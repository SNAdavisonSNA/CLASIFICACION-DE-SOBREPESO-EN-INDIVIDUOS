{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f780389a",
   "metadata": {},
   "source": [
    "## Regresión logística multinomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd2168",
   "metadata": {},
   "source": [
    "# Leer set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5092d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2111 entries, 0 to 2110\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Gender                          2111 non-null   object \n",
      " 1   Age                             2111 non-null   float64\n",
      " 2   Height                          2111 non-null   float64\n",
      " 3   Weight                          2111 non-null   float64\n",
      " 4   family_history_with_overweight  2111 non-null   object \n",
      " 5   FAVC                            2111 non-null   object \n",
      " 6   FCVC                            2111 non-null   float64\n",
      " 7   NCP                             2111 non-null   float64\n",
      " 8   CAEC                            2111 non-null   object \n",
      " 9   SMOKE                           2111 non-null   object \n",
      " 10  CH2O                            2111 non-null   float64\n",
      " 11  SCC                             2111 non-null   object \n",
      " 12  FAF                             2111 non-null   float64\n",
      " 13  TUE                             2111 non-null   float64\n",
      " 14  CALC                            2111 non-null   object \n",
      " 15  MTRANS                          2111 non-null   object \n",
      " 16  NObeyesdad                      2111 non-null   object \n",
      "dtypes: float64(8), object(9)\n",
      "memory usage: 280.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Desactivar todas las warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(\"ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b0763",
   "metadata": {},
   "source": [
    "# El set de datos no contiene valores vacíos, por lo que se aplicará tratamiento a las columnas para tener información estandarizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cf6cf",
   "metadata": {},
   "source": [
    "## Utilizar Label Encoder para variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64eddac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2111 entries, 0 to 2110\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Gender                          2111 non-null   int32  \n",
      " 1   Age                             2111 non-null   float64\n",
      " 2   Height                          2111 non-null   float64\n",
      " 3   Weight                          2111 non-null   float64\n",
      " 4   family_history_with_overweight  2111 non-null   int32  \n",
      " 5   FAVC                            2111 non-null   int32  \n",
      " 6   FCVC                            2111 non-null   float64\n",
      " 7   NCP                             2111 non-null   float64\n",
      " 8   CAEC                            2111 non-null   int32  \n",
      " 9   SMOKE                           2111 non-null   int32  \n",
      " 10  CH2O                            2111 non-null   float64\n",
      " 11  SCC                             2111 non-null   int32  \n",
      " 12  FAF                             2111 non-null   float64\n",
      " 13  TUE                             2111 non-null   float64\n",
      " 14  CALC                            2111 non-null   int32  \n",
      " 15  MTRANS                          2111 non-null   int32  \n",
      " 16  NObeyesdad                      2111 non-null   int32  \n",
      "dtypes: float64(8), int32(9)\n",
      "memory usage: 206.3 KB\n"
     ]
    }
   ],
   "source": [
    "#Reemplazar valores binarios (yes/no - Female/Male) por valores binarios númericos (1/0)\n",
    "\n",
    "#df = df.replace(['no', 'yes'], [0, 1])\n",
    "#df = df.replace(['Female', 'Male'], [0, 1])\n",
    "\n",
    "\n",
    "#Asignar variables categóricas\n",
    "#df['CAEC'] = df['CAEC'].replace(['Sometimes', 'Frequently', 'Always'], [1, 2, 3])\n",
    "#df['CALC'] = df['CALC'].replace(['Sometimes', 'Frequently', 'Always'], [1, 2, 3])\n",
    "#df['MTRANS'] = df['MTRANS'].replace(['Automobile', 'Motorbike', 'Bike', 'Public_Transportation', 'Walking'], [1, 2, 3, 4, 5])\n",
    "#df['NObeyesdad'] = df['NObeyesdad'].replace(['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III'], [1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "#Utilizando label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Lista de columnas categóricas\n",
    "categorical_columns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"CAEC\", \"SMOKE\", \"SCC\", \n",
    "                       \"CALC\", \"MTRANS\", \"NObeyesdad\"]\n",
    "\n",
    "# Crear una instancia de LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Aplicar LabelEncoder a cada columna categórica\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Verificar los resultados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e4fba",
   "metadata": {},
   "source": [
    "## Ver correlación entre cada columna respecto a la variable objetivo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76e637e3",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcula la matriz de correlación\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Crea un heatmap con seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af158f",
   "metadata": {},
   "source": [
    "## Revisar que la variable objetivo esté balanceada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2ece0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    351\n",
      "4    324\n",
      "3    297\n",
      "5    290\n",
      "6    290\n",
      "1    287\n",
      "0    272\n",
      "Name: NObeyesdad, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Distribución de Clases'}>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calcular la cuenta de valores únicos en la columna 'Clase'\n",
    "value_counts = df['NObeyesdad'].value_counts()\n",
    "\n",
    "# Mostrar la cuenta de valores únicos\n",
    "print(value_counts)\n",
    "\n",
    "# Visualización de la distribución de clases\n",
    "value_counts.plot(kind='bar', title='Distribución de Clases')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a762fab",
   "metadata": {},
   "source": [
    "# Utilizar distintos modelos dentro de cada pipeline y evaluar cual da mejores resultados con distintas configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fbb5b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando Pipeline: Logistic Regression\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Mejores parámetros para Logistic Regression: {'classifier__C': 100}\n",
      "Exactitud en el conjunto de prueba para Logistic Regression: 0.9598108747044918\n",
      "\n",
      "Evaluando Pipeline: SVM\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Mejores parámetros para SVM: {'classifier__C': 100, 'classifier__gamma': 0.01}\n",
      "Exactitud en el conjunto de prueba para SVM: 0.9621749408983451\n",
      "\n",
      "Evaluando Pipeline: Random Forest\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Mejores parámetros para Random Forest: {'classifier__max_depth': 20, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Exactitud en el conjunto de prueba para Random Forest: 0.9574468085106383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir los clasificadores y pipelines\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "pipelines = {}\n",
    "\n",
    "# Crear los pipelines para cada clasificador\n",
    "for name, classifier in classifiers.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "# Definir parámetros para GridSearch de cada pipeline\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__gamma': [0.001, 0.01, 0.1, 1]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Almacenar los resultados en un DataFrame\n",
    "results = []\n",
    "\n",
    "# Realizar GridSearch para cada pipeline\n",
    "for name, pipeline in pipelines.items():\n",
    "    print(f\"Evaluando Pipeline: {name}\")\n",
    "\n",
    "    # Obtener el espacio de parámetros correspondiente\n",
    "    param_grid = param_grids[name]\n",
    "\n",
    "    # Aplicar GridSearch\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Almacenar los resultados en un DataFrame\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_df['Classifier'] = name\n",
    "    results.append(results_df)\n",
    "\n",
    "    # Imprimir los mejores parámetros encontrados en GridSearch\n",
    "    print(f\"Mejores parámetros para {name}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluar el rendimiento en el conjunto de prueba\n",
    "    accuracy = grid_search.score(X_test, y_test)\n",
    "    print(f\"Exactitud en el conjunto de prueba para {name}: {accuracy}\")\n",
    "    print()\n",
    "\n",
    "# Combinar los resultados en un único DataFrame\n",
    "results_df = pd.concat(results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "024e85a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>Classifier</th>\n",
       "      <th>param_classifier__gamma</th>\n",
       "      <th>param_classifier__max_depth</th>\n",
       "      <th>param_classifier__min_samples_split</th>\n",
       "      <th>param_classifier__n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.003177</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'classifier__C': 0.001}</td>\n",
       "      <td>0.428994</td>\n",
       "      <td>0.411243</td>\n",
       "      <td>0.393491</td>\n",
       "      <td>0.421365</td>\n",
       "      <td>0.400593</td>\n",
       "      <td>0.411137</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>6</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'classifier__C': 0.01}</td>\n",
       "      <td>0.588757</td>\n",
       "      <td>0.559172</td>\n",
       "      <td>0.562130</td>\n",
       "      <td>0.566766</td>\n",
       "      <td>0.551929</td>\n",
       "      <td>0.565751</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035904</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'classifier__C': 0.1}</td>\n",
       "      <td>0.698225</td>\n",
       "      <td>0.710059</td>\n",
       "      <td>0.677515</td>\n",
       "      <td>0.691395</td>\n",
       "      <td>0.694362</td>\n",
       "      <td>0.694311</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>4</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.071808</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__C': 1}</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.822485</td>\n",
       "      <td>0.825444</td>\n",
       "      <td>0.810089</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.823449</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>3</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__C': 10}</td>\n",
       "      <td>0.917160</td>\n",
       "      <td>0.914201</td>\n",
       "      <td>0.890533</td>\n",
       "      <td>0.893175</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>0.909957</td>\n",
       "      <td>0.016383</td>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.073604</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier__C': 100}</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.919881</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.934839</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>1</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.045678</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'classifier__C': 0.001, 'classifier__gamma': ...</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.144215</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.045480</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'classifier__C': 0.001, 'classifier__gamma': ...</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.144612</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.045080</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'classifier__C': 0.001, 'classifier__gamma': ...</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.140424</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.044481</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'classifier__C': 0.001, 'classifier__gamma': 1}</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.142826</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__gamma': 0...</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.145010</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.045074</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__gamma': 0...</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.142420</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.044880</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__gamma': 0.1}</td>\n",
       "      <td>0.304734</td>\n",
       "      <td>0.316568</td>\n",
       "      <td>0.316568</td>\n",
       "      <td>0.314540</td>\n",
       "      <td>0.308605</td>\n",
       "      <td>0.312203</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>16</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.143622</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'classifier__C': 0.01, 'classifier__gamma': 1}</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.143225</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.044866</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__gamma': 0....</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.162722</td>\n",
       "      <td>0.163205</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.161730</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>17</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.129071</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.042883</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__gamma': 0.01}</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>0.411243</td>\n",
       "      <td>0.355030</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.382789</td>\n",
       "      <td>0.373801</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>15</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.101531</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.040689</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__gamma': 0.1}</td>\n",
       "      <td>0.606509</td>\n",
       "      <td>0.647929</td>\n",
       "      <td>0.603550</td>\n",
       "      <td>0.623145</td>\n",
       "      <td>0.599407</td>\n",
       "      <td>0.616108</td>\n",
       "      <td>0.017837</td>\n",
       "      <td>12</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.124661</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.039892</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'classifier__C': 0.1, 'classifier__gamma': 1}</td>\n",
       "      <td>0.440828</td>\n",
       "      <td>0.408284</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.418398</td>\n",
       "      <td>0.400593</td>\n",
       "      <td>0.418236</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>13</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.137431</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.045676</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__gamma': 0.001}</td>\n",
       "      <td>0.405325</td>\n",
       "      <td>0.420118</td>\n",
       "      <td>0.369822</td>\n",
       "      <td>0.367953</td>\n",
       "      <td>0.406528</td>\n",
       "      <td>0.393949</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>14</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.087969</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.039498</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__gamma': 0.01}</td>\n",
       "      <td>0.674556</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.659763</td>\n",
       "      <td>0.700297</td>\n",
       "      <td>0.673591</td>\n",
       "      <td>0.680103</td>\n",
       "      <td>0.014447</td>\n",
       "      <td>10</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.035116</td>\n",
       "      <td>0.005213</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__gamma': 0.1}</td>\n",
       "      <td>0.866864</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.881657</td>\n",
       "      <td>0.830861</td>\n",
       "      <td>0.869436</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>6</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.138612</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.033929</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier__C': 1, 'classifier__gamma': 1}</td>\n",
       "      <td>0.855030</td>\n",
       "      <td>0.784024</td>\n",
       "      <td>0.872781</td>\n",
       "      <td>0.821958</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.830557</td>\n",
       "      <td>0.030838</td>\n",
       "      <td>9</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.089804</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.040086</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__gamma': 0.001}</td>\n",
       "      <td>0.665680</td>\n",
       "      <td>0.668639</td>\n",
       "      <td>0.630178</td>\n",
       "      <td>0.697329</td>\n",
       "      <td>0.635015</td>\n",
       "      <td>0.659368</td>\n",
       "      <td>0.024544</td>\n",
       "      <td>11</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.056242</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__gamma': 0.01}</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.890533</td>\n",
       "      <td>0.881657</td>\n",
       "      <td>0.884273</td>\n",
       "      <td>0.905045</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>4</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.060046</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.025724</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__gamma': 0.1}</td>\n",
       "      <td>0.911243</td>\n",
       "      <td>0.914201</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.893175</td>\n",
       "      <td>0.937685</td>\n",
       "      <td>0.916468</td>\n",
       "      <td>0.014945</td>\n",
       "      <td>3</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.130046</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.032924</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>10</td>\n",
       "      <td>{'classifier__C': 10, 'classifier__gamma': 1}</td>\n",
       "      <td>0.866864</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.836795</td>\n",
       "      <td>0.836795</td>\n",
       "      <td>0.844186</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>7</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.058040</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.029718</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier__C': 100, 'classifier__gamma': 0....</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.857988</td>\n",
       "      <td>0.875740</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.902077</td>\n",
       "      <td>0.880336</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>5</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.051462</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.019556</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier__C': 100, 'classifier__gamma': 0.01}</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>0.970326</td>\n",
       "      <td>0.949648</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>1</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier__C': 100, 'classifier__gamma': 0.1}</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.893175</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.923574</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>2</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.126461</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.031518</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>100</td>\n",
       "      <td>{'classifier__C': 100, 'classifier__gamma': 1}</td>\n",
       "      <td>0.866864</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.887574</td>\n",
       "      <td>0.836795</td>\n",
       "      <td>0.836795</td>\n",
       "      <td>0.844186</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>7</td>\n",
       "      <td>SVM</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.151597</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.942553</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>19</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.290625</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.013561</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.937870</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.947873</td>\n",
       "      <td>0.009071</td>\n",
       "      <td>10</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.577067</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.950831</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>5</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.143617</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>0.011372</td>\n",
       "      <td>6</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.283652</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.949056</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>7</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.618945</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.026726</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>0.009991</td>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.148793</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.938992</td>\n",
       "      <td>0.013545</td>\n",
       "      <td>24</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.289721</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.937685</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.942542</td>\n",
       "      <td>0.012479</td>\n",
       "      <td>20</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.567487</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.025727</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': None, 'classifier__m...</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.943141</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>18</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.146199</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.937685</td>\n",
       "      <td>0.955490</td>\n",
       "      <td>0.938990</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>25</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.287632</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.013969</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.937870</td>\n",
       "      <td>0.937685</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.008463</td>\n",
       "      <td>21</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.569087</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.025728</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.946099</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>13</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.142826</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.008364</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.944321</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>17</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.287431</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.013764</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.931751</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.946089</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>14</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.574648</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.025738</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.952663</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.945504</td>\n",
       "      <td>0.010332</td>\n",
       "      <td>16</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.149009</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.919881</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.936023</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>27</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.297206</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.015366</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>0.952522</td>\n",
       "      <td>0.937211</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>26</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.547939</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.025527</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 10, 'classifier__min...</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.928783</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.940765</td>\n",
       "      <td>0.014494</td>\n",
       "      <td>22</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.958457</td>\n",
       "      <td>0.946686</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>12</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.295609</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.964497</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.952608</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>1</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.601995</td>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.025929</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.937870</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.950833</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>4</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.151205</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.937870</td>\n",
       "      <td>0.943620</td>\n",
       "      <td>0.961424</td>\n",
       "      <td>0.945506</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>15</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.292218</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.947873</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>10</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.597805</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.026127</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.952663</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.952020</td>\n",
       "      <td>0.012317</td>\n",
       "      <td>2</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.142074</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.934718</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.940179</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>23</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.298198</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.943787</td>\n",
       "      <td>0.958580</td>\n",
       "      <td>0.937685</td>\n",
       "      <td>0.964392</td>\n",
       "      <td>0.949054</td>\n",
       "      <td>0.010494</td>\n",
       "      <td>8</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.554323</td>\n",
       "      <td>0.011526</td>\n",
       "      <td>0.024920</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'classifier__max_depth': 20, 'classifier__min...</td>\n",
       "      <td>0.931953</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.946588</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>9</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.017156      0.003177         0.001993        0.000634   \n",
       "1        0.017952      0.002092         0.001995        0.000631   \n",
       "2        0.035904      0.003568         0.002394        0.001017   \n",
       "3        0.071808      0.006308         0.002593        0.001017   \n",
       "4        0.066223      0.005520         0.003791        0.000747   \n",
       "5        0.073604      0.008062         0.002991        0.000631   \n",
       "6        0.144011      0.009114         0.045678        0.001732   \n",
       "7        0.144215      0.001016         0.045480        0.000485   \n",
       "8        0.144612      0.001892         0.045080        0.000977   \n",
       "9        0.140424      0.002393         0.044481        0.001017   \n",
       "10       0.142826      0.004661         0.045485        0.001359   \n",
       "11       0.145010      0.001018         0.045074        0.000739   \n",
       "12       0.142420      0.003306         0.044880        0.001090   \n",
       "13       0.143622      0.005174         0.045273        0.001845   \n",
       "14       0.143225      0.003611         0.044866        0.000620   \n",
       "15       0.129071      0.001026         0.042883        0.000003   \n",
       "16       0.101531      0.001600         0.040689        0.001146   \n",
       "17       0.124661      0.000620         0.039892        0.000021   \n",
       "18       0.137431      0.005465         0.045676        0.001164   \n",
       "19       0.087969      0.002643         0.039498        0.000491   \n",
       "20       0.065433      0.002649         0.035116        0.005213   \n",
       "21       0.138612      0.001258         0.033929        0.000620   \n",
       "22       0.089804      0.002586         0.040086        0.000979   \n",
       "23       0.056242      0.000793         0.029914        0.001098   \n",
       "24       0.060046      0.002027         0.025724        0.000735   \n",
       "25       0.130046      0.001372         0.032924        0.000907   \n",
       "26       0.058040      0.000747         0.029718        0.000741   \n",
       "27       0.051462      0.000797         0.019556        0.000479   \n",
       "28       0.056448      0.000487         0.023139        0.000748   \n",
       "29       0.126461      0.001323         0.031518        0.000489   \n",
       "30       0.151597      0.002094         0.007775        0.000396   \n",
       "31       0.290625      0.002060         0.013561        0.000495   \n",
       "32       0.577067      0.001011         0.025723        0.000398   \n",
       "33       0.143617      0.001543         0.007778        0.000399   \n",
       "34       0.283652      0.002234         0.013555        0.000490   \n",
       "35       0.618945      0.021025         0.026726        0.000740   \n",
       "36       0.148793      0.001707         0.007979        0.000021   \n",
       "37       0.289721      0.006095         0.013578        0.000499   \n",
       "38       0.567487      0.013703         0.025727        0.000745   \n",
       "39       0.146199      0.003821         0.007981        0.000021   \n",
       "40       0.287632      0.004212         0.013969        0.000025   \n",
       "41       0.569087      0.002256         0.025728        0.000754   \n",
       "42       0.142826      0.001597         0.008364        0.000485   \n",
       "43       0.287431      0.003965         0.013764        0.000399   \n",
       "44       0.574648      0.002921         0.025738        0.000403   \n",
       "45       0.149009      0.011027         0.008171        0.000403   \n",
       "46       0.297206      0.011941         0.015366        0.002789   \n",
       "47       0.547939      0.011531         0.025527        0.000486   \n",
       "48       0.150206      0.002928         0.007978        0.000003   \n",
       "49       0.295609      0.004588         0.013763        0.000399   \n",
       "50       0.601995      0.006822         0.025929        0.000631   \n",
       "51       0.151205      0.003724         0.007979        0.000002   \n",
       "52       0.292218      0.005481         0.014155        0.000387   \n",
       "53       0.597805      0.015752         0.026127        0.000401   \n",
       "54       0.142074      0.002868         0.007575        0.000493   \n",
       "55       0.298198      0.026671         0.013775        0.000758   \n",
       "56       0.554323      0.011526         0.024920        0.000616   \n",
       "\n",
       "   param_classifier__C                                             params  \\\n",
       "0                0.001                           {'classifier__C': 0.001}   \n",
       "1                 0.01                            {'classifier__C': 0.01}   \n",
       "2                  0.1                             {'classifier__C': 0.1}   \n",
       "3                    1                               {'classifier__C': 1}   \n",
       "4                   10                              {'classifier__C': 10}   \n",
       "5                  100                             {'classifier__C': 100}   \n",
       "6                0.001  {'classifier__C': 0.001, 'classifier__gamma': ...   \n",
       "7                0.001  {'classifier__C': 0.001, 'classifier__gamma': ...   \n",
       "8                0.001  {'classifier__C': 0.001, 'classifier__gamma': ...   \n",
       "9                0.001   {'classifier__C': 0.001, 'classifier__gamma': 1}   \n",
       "10                0.01  {'classifier__C': 0.01, 'classifier__gamma': 0...   \n",
       "11                0.01  {'classifier__C': 0.01, 'classifier__gamma': 0...   \n",
       "12                0.01  {'classifier__C': 0.01, 'classifier__gamma': 0.1}   \n",
       "13                0.01    {'classifier__C': 0.01, 'classifier__gamma': 1}   \n",
       "14                 0.1  {'classifier__C': 0.1, 'classifier__gamma': 0....   \n",
       "15                 0.1  {'classifier__C': 0.1, 'classifier__gamma': 0.01}   \n",
       "16                 0.1   {'classifier__C': 0.1, 'classifier__gamma': 0.1}   \n",
       "17                 0.1     {'classifier__C': 0.1, 'classifier__gamma': 1}   \n",
       "18                   1   {'classifier__C': 1, 'classifier__gamma': 0.001}   \n",
       "19                   1    {'classifier__C': 1, 'classifier__gamma': 0.01}   \n",
       "20                   1     {'classifier__C': 1, 'classifier__gamma': 0.1}   \n",
       "21                   1       {'classifier__C': 1, 'classifier__gamma': 1}   \n",
       "22                  10  {'classifier__C': 10, 'classifier__gamma': 0.001}   \n",
       "23                  10   {'classifier__C': 10, 'classifier__gamma': 0.01}   \n",
       "24                  10    {'classifier__C': 10, 'classifier__gamma': 0.1}   \n",
       "25                  10      {'classifier__C': 10, 'classifier__gamma': 1}   \n",
       "26                 100  {'classifier__C': 100, 'classifier__gamma': 0....   \n",
       "27                 100  {'classifier__C': 100, 'classifier__gamma': 0.01}   \n",
       "28                 100   {'classifier__C': 100, 'classifier__gamma': 0.1}   \n",
       "29                 100     {'classifier__C': 100, 'classifier__gamma': 1}   \n",
       "30                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "31                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "32                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "33                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "34                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "35                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "36                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "37                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "38                 NaN  {'classifier__max_depth': None, 'classifier__m...   \n",
       "39                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "40                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "41                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "42                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "43                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "44                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "45                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "46                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "47                 NaN  {'classifier__max_depth': 10, 'classifier__min...   \n",
       "48                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "49                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "50                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "51                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "52                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "53                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "54                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "55                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "56                 NaN  {'classifier__max_depth': 20, 'classifier__min...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.428994           0.411243           0.393491   \n",
       "1            0.588757           0.559172           0.562130   \n",
       "2            0.698225           0.710059           0.677515   \n",
       "3            0.840237           0.822485           0.825444   \n",
       "4            0.917160           0.914201           0.890533   \n",
       "5            0.946746           0.923077           0.926036   \n",
       "6            0.159763           0.162722           0.162722   \n",
       "7            0.159763           0.162722           0.162722   \n",
       "8            0.159763           0.162722           0.162722   \n",
       "9            0.159763           0.162722           0.162722   \n",
       "10           0.159763           0.162722           0.162722   \n",
       "11           0.159763           0.162722           0.162722   \n",
       "12           0.304734           0.316568           0.316568   \n",
       "13           0.159763           0.162722           0.162722   \n",
       "14           0.159763           0.162722           0.162722   \n",
       "15           0.378698           0.411243           0.355030   \n",
       "16           0.606509           0.647929           0.603550   \n",
       "17           0.440828           0.408284           0.423077   \n",
       "18           0.405325           0.420118           0.369822   \n",
       "19           0.674556           0.692308           0.659763   \n",
       "20           0.866864           0.846154           0.881657   \n",
       "21           0.855030           0.784024           0.872781   \n",
       "22           0.665680           0.668639           0.630178   \n",
       "23           0.893491           0.890533           0.881657   \n",
       "24           0.911243           0.914201           0.926036   \n",
       "25           0.866864           0.792899           0.887574   \n",
       "26           0.899408           0.857988           0.875740   \n",
       "27           0.961538           0.949704           0.931953   \n",
       "28           0.931953           0.905325           0.940828   \n",
       "29           0.866864           0.792899           0.887574   \n",
       "30           0.923077           0.928994           0.946746   \n",
       "31           0.937870           0.940828           0.955621   \n",
       "32           0.943787           0.949704           0.955621   \n",
       "33           0.934911           0.943787           0.961538   \n",
       "34           0.940828           0.943787           0.955621   \n",
       "35           0.943787           0.940828           0.958580   \n",
       "36           0.926036           0.926036           0.946746   \n",
       "37           0.934911           0.926036           0.955621   \n",
       "38           0.928994           0.934911           0.943787   \n",
       "39           0.928994           0.931953           0.940828   \n",
       "40           0.940828           0.934911           0.937870   \n",
       "41           0.931953           0.931953           0.958580   \n",
       "42           0.931953           0.946746           0.940828   \n",
       "43           0.934911           0.943787           0.961538   \n",
       "44           0.931953           0.940828           0.952663   \n",
       "45           0.928994           0.923077           0.949704   \n",
       "46           0.931953           0.926036           0.940828   \n",
       "47           0.923077           0.940828           0.946746   \n",
       "48           0.934911           0.940828           0.958580   \n",
       "49           0.949704           0.940828           0.964497   \n",
       "50           0.937870           0.949704           0.958580   \n",
       "51           0.940828           0.943787           0.937870   \n",
       "52           0.931953           0.946746           0.955621   \n",
       "53           0.931953           0.952663           0.961538   \n",
       "54           0.934911           0.931953           0.931953   \n",
       "55           0.940828           0.943787           0.958580   \n",
       "56           0.931953           0.940828           0.955621   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.421365           0.400593         0.411137        0.013008   \n",
       "1            0.566766           0.551929         0.565751        0.012472   \n",
       "2            0.691395           0.694362         0.694311        0.010526   \n",
       "3            0.810089           0.818991         0.823449        0.009850   \n",
       "4            0.893175           0.934718         0.909957        0.016383   \n",
       "5            0.919881           0.958457         0.934839        0.015094   \n",
       "6            0.163205           0.160237         0.161730        0.001431   \n",
       "7            0.163205           0.160237         0.161730        0.001431   \n",
       "8            0.163205           0.160237         0.161730        0.001431   \n",
       "9            0.163205           0.160237         0.161730        0.001431   \n",
       "10           0.163205           0.160237         0.161730        0.001431   \n",
       "11           0.163205           0.160237         0.161730        0.001431   \n",
       "12           0.314540           0.308605         0.312203        0.004739   \n",
       "13           0.163205           0.160237         0.161730        0.001431   \n",
       "14           0.163205           0.160237         0.161730        0.001431   \n",
       "15           0.341246           0.382789         0.373801        0.024161   \n",
       "16           0.623145           0.599407         0.616108        0.017837   \n",
       "17           0.418398           0.400593         0.418236        0.013742   \n",
       "18           0.367953           0.406528         0.393949        0.021121   \n",
       "19           0.700297           0.673591         0.680103        0.014447   \n",
       "20           0.830861           0.869436         0.858994        0.018116   \n",
       "21           0.821958           0.818991         0.830557        0.030838   \n",
       "22           0.697329           0.635015         0.659368        0.024544   \n",
       "23           0.884273           0.905045         0.891000        0.008200   \n",
       "24           0.893175           0.937685         0.916468        0.014945   \n",
       "25           0.836795           0.836795         0.844186        0.032051   \n",
       "26           0.866469           0.902077         0.880336        0.017603   \n",
       "27           0.934718           0.970326         0.949648        0.014866   \n",
       "28           0.893175           0.946588         0.923574        0.020759   \n",
       "29           0.836795           0.836795         0.844186        0.032051   \n",
       "30           0.946588           0.967359         0.942553        0.015571   \n",
       "31           0.943620           0.961424         0.947873        0.009071   \n",
       "32           0.943620           0.961424         0.950831        0.006900   \n",
       "33           0.943620           0.964392         0.949650        0.011372   \n",
       "34           0.940653           0.964392         0.949056        0.009427   \n",
       "35           0.946588           0.967359         0.951428        0.009991   \n",
       "36           0.934718           0.961424         0.938992        0.013545   \n",
       "37           0.937685           0.958457         0.942542        0.012479   \n",
       "38           0.943620           0.964392         0.943141        0.012001   \n",
       "39           0.937685           0.955490         0.938990        0.009239   \n",
       "40           0.937685           0.958457         0.941950        0.008463   \n",
       "41           0.946588           0.961424         0.946099        0.012579   \n",
       "42           0.940653           0.961424         0.944321        0.009768   \n",
       "43           0.931751           0.958457         0.946089        0.012062   \n",
       "44           0.940653           0.961424         0.945504        0.010332   \n",
       "45           0.919881           0.958457         0.936023        0.015284   \n",
       "46           0.934718           0.952522         0.937211        0.009014   \n",
       "47           0.928783           0.964392         0.940765        0.014494   \n",
       "48           0.940653           0.958457         0.946686        0.009893   \n",
       "49           0.946588           0.961424         0.952608        0.008972   \n",
       "50           0.943620           0.964392         0.950833        0.009646   \n",
       "51           0.943620           0.961424         0.945506        0.008247   \n",
       "52           0.940653           0.964392         0.947873        0.011312   \n",
       "53           0.946588           0.967359         0.952020        0.012317   \n",
       "54           0.934718           0.967359         0.940179        0.013650   \n",
       "55           0.937685           0.964392         0.949054        0.010494   \n",
       "56           0.946588           0.967359         0.948470        0.012188   \n",
       "\n",
       "    rank_test_score           Classifier param_classifier__gamma  \\\n",
       "0                 6  Logistic Regression                     NaN   \n",
       "1                 5  Logistic Regression                     NaN   \n",
       "2                 4  Logistic Regression                     NaN   \n",
       "3                 3  Logistic Regression                     NaN   \n",
       "4                 2  Logistic Regression                     NaN   \n",
       "5                 1  Logistic Regression                     NaN   \n",
       "6                17                  SVM                   0.001   \n",
       "7                17                  SVM                    0.01   \n",
       "8                17                  SVM                     0.1   \n",
       "9                17                  SVM                       1   \n",
       "10               17                  SVM                   0.001   \n",
       "11               17                  SVM                    0.01   \n",
       "12               16                  SVM                     0.1   \n",
       "13               17                  SVM                       1   \n",
       "14               17                  SVM                   0.001   \n",
       "15               15                  SVM                    0.01   \n",
       "16               12                  SVM                     0.1   \n",
       "17               13                  SVM                       1   \n",
       "18               14                  SVM                   0.001   \n",
       "19               10                  SVM                    0.01   \n",
       "20                6                  SVM                     0.1   \n",
       "21                9                  SVM                       1   \n",
       "22               11                  SVM                   0.001   \n",
       "23                4                  SVM                    0.01   \n",
       "24                3                  SVM                     0.1   \n",
       "25                7                  SVM                       1   \n",
       "26                5                  SVM                   0.001   \n",
       "27                1                  SVM                    0.01   \n",
       "28                2                  SVM                     0.1   \n",
       "29                7                  SVM                       1   \n",
       "30               19        Random Forest                     NaN   \n",
       "31               10        Random Forest                     NaN   \n",
       "32                5        Random Forest                     NaN   \n",
       "33                6        Random Forest                     NaN   \n",
       "34                7        Random Forest                     NaN   \n",
       "35                3        Random Forest                     NaN   \n",
       "36               24        Random Forest                     NaN   \n",
       "37               20        Random Forest                     NaN   \n",
       "38               18        Random Forest                     NaN   \n",
       "39               25        Random Forest                     NaN   \n",
       "40               21        Random Forest                     NaN   \n",
       "41               13        Random Forest                     NaN   \n",
       "42               17        Random Forest                     NaN   \n",
       "43               14        Random Forest                     NaN   \n",
       "44               16        Random Forest                     NaN   \n",
       "45               27        Random Forest                     NaN   \n",
       "46               26        Random Forest                     NaN   \n",
       "47               22        Random Forest                     NaN   \n",
       "48               12        Random Forest                     NaN   \n",
       "49                1        Random Forest                     NaN   \n",
       "50                4        Random Forest                     NaN   \n",
       "51               15        Random Forest                     NaN   \n",
       "52               10        Random Forest                     NaN   \n",
       "53                2        Random Forest                     NaN   \n",
       "54               23        Random Forest                     NaN   \n",
       "55                8        Random Forest                     NaN   \n",
       "56                9        Random Forest                     NaN   \n",
       "\n",
       "   param_classifier__max_depth param_classifier__min_samples_split  \\\n",
       "0                          NaN                                 NaN   \n",
       "1                          NaN                                 NaN   \n",
       "2                          NaN                                 NaN   \n",
       "3                          NaN                                 NaN   \n",
       "4                          NaN                                 NaN   \n",
       "5                          NaN                                 NaN   \n",
       "6                          NaN                                 NaN   \n",
       "7                          NaN                                 NaN   \n",
       "8                          NaN                                 NaN   \n",
       "9                          NaN                                 NaN   \n",
       "10                         NaN                                 NaN   \n",
       "11                         NaN                                 NaN   \n",
       "12                         NaN                                 NaN   \n",
       "13                         NaN                                 NaN   \n",
       "14                         NaN                                 NaN   \n",
       "15                         NaN                                 NaN   \n",
       "16                         NaN                                 NaN   \n",
       "17                         NaN                                 NaN   \n",
       "18                         NaN                                 NaN   \n",
       "19                         NaN                                 NaN   \n",
       "20                         NaN                                 NaN   \n",
       "21                         NaN                                 NaN   \n",
       "22                         NaN                                 NaN   \n",
       "23                         NaN                                 NaN   \n",
       "24                         NaN                                 NaN   \n",
       "25                         NaN                                 NaN   \n",
       "26                         NaN                                 NaN   \n",
       "27                         NaN                                 NaN   \n",
       "28                         NaN                                 NaN   \n",
       "29                         NaN                                 NaN   \n",
       "30                        None                                   2   \n",
       "31                        None                                   2   \n",
       "32                        None                                   2   \n",
       "33                        None                                   5   \n",
       "34                        None                                   5   \n",
       "35                        None                                   5   \n",
       "36                        None                                  10   \n",
       "37                        None                                  10   \n",
       "38                        None                                  10   \n",
       "39                          10                                   2   \n",
       "40                          10                                   2   \n",
       "41                          10                                   2   \n",
       "42                          10                                   5   \n",
       "43                          10                                   5   \n",
       "44                          10                                   5   \n",
       "45                          10                                  10   \n",
       "46                          10                                  10   \n",
       "47                          10                                  10   \n",
       "48                          20                                   2   \n",
       "49                          20                                   2   \n",
       "50                          20                                   2   \n",
       "51                          20                                   5   \n",
       "52                          20                                   5   \n",
       "53                          20                                   5   \n",
       "54                          20                                  10   \n",
       "55                          20                                  10   \n",
       "56                          20                                  10   \n",
       "\n",
       "   param_classifier__n_estimators  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4                             NaN  \n",
       "5                             NaN  \n",
       "6                             NaN  \n",
       "7                             NaN  \n",
       "8                             NaN  \n",
       "9                             NaN  \n",
       "10                            NaN  \n",
       "11                            NaN  \n",
       "12                            NaN  \n",
       "13                            NaN  \n",
       "14                            NaN  \n",
       "15                            NaN  \n",
       "16                            NaN  \n",
       "17                            NaN  \n",
       "18                            NaN  \n",
       "19                            NaN  \n",
       "20                            NaN  \n",
       "21                            NaN  \n",
       "22                            NaN  \n",
       "23                            NaN  \n",
       "24                            NaN  \n",
       "25                            NaN  \n",
       "26                            NaN  \n",
       "27                            NaN  \n",
       "28                            NaN  \n",
       "29                            NaN  \n",
       "30                             50  \n",
       "31                            100  \n",
       "32                            200  \n",
       "33                             50  \n",
       "34                            100  \n",
       "35                            200  \n",
       "36                             50  \n",
       "37                            100  \n",
       "38                            200  \n",
       "39                             50  \n",
       "40                            100  \n",
       "41                            200  \n",
       "42                             50  \n",
       "43                            100  \n",
       "44                            200  \n",
       "45                             50  \n",
       "46                            100  \n",
       "47                            200  \n",
       "48                             50  \n",
       "49                            100  \n",
       "50                            200  \n",
       "51                             50  \n",
       "52                            100  \n",
       "53                            200  \n",
       "54                             50  \n",
       "55                            100  \n",
       "56                            200  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar el DataFrame con los resultados (para ver el contenido del df obtenido de .cv_results_)\n",
    "results_df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba2599",
   "metadata": {},
   "source": [
    "## Graficar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "54310e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtrar resultados para obtener solo las columnas relevantes\n",
    "relevant_columns = ['Classifier', 'params', 'mean_test_score']\n",
    "filtered_results = results_df[relevant_columns]\n",
    "\n",
    "# Encontrar el mejor resultado para cada clasificador\n",
    "best_results = filtered_results.groupby('Classifier').apply(lambda x: x[x['mean_test_score'] == x['mean_test_score'].max()])\n",
    "\n",
    "# Ordenar los resultados por precisión\n",
    "best_results = best_results.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Crear un gráfico de barras horizontal\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='mean_test_score', y='Classifier', data=best_results, palette='viridis')\n",
    "plt.title('Mejor Precisión para Cada Modelo')\n",
    "plt.xlabel('Precisión Media en CV (cross-validation)')\n",
    "plt.ylabel('Modelo')\n",
    "\n",
    "# Anotar las barras con los valores de precisión\n",
    "for index, value in enumerate(best_results['mean_test_score']):\n",
    "    plt.text(value, index, f'{value:.4f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef31ff2",
   "metadata": {},
   "source": [
    "# Conclusión: Discrepancia en accuracy de evaluación con pipeline y GridSearch vs accuracy en graficado\n",
    "\n",
    "### El resultado del evaluado con los parametros seleccionados por GridSearch muestra un mejor desempeño para SVM con un accuracy de 0.9622, mientras que en el graficado resulta que Random Forest tiene un accuracy medio más alto con 0.9538 (a comparación de los resultados mostrados en la grafica).\n",
    "\n",
    "### Esto puede deberse a que en la evaluación con GridSearch, el modelo SVM con los parametros con mejor desempeño dan ese resultado que es más alto comparado con el mismo proceso de los otros modelos. Mientras que en el graficado se está utilizando el accuracy más alto de cada combinación distinta de parametros, lo que hace que Random Forest tenga un accuracy medio más alto, esto debido  que puede contener datos con un accuracy más alto y constante, lo que eleva su precisión media de este valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9459bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
